{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running llama.cpp with VDS Tunnel in Google Colab\n",
    "\n",
    "## Настройка\n",
    "\n",
    "1. В Colab добавьте секреты (значок ключа слева):\n",
    "   - vds_host: IP вашего VDS\n",
    "   - ssh_key: приватный SSH-ключ в base64 (конвертируйте командой `base64 -w0 ~/.ssh/id_rsa`)\n",
    "\n",
    "2. На VDS:\n",
    "   - Добавьте публичный ключ в `~/.ssh/authorized_keys`\n",
    "   - Откройте порт 8000\n",
    "   - Разрешите создание обратного туннеля в `/etc/ssh/sshd_config`: `GatewayPorts yes`\n",
    "\n",
    "3. Используйте GPU runtime (Runtime -> Change runtime type -> GPU)\n",
    "\n",
    "## 1. Установка llama-cpp-python с поддержкой CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка llama-cpp-python с поддержкой CUDA\n",
    "import os\n",
    "os.environ['CMAKE_ARGS'] = '-DGGML_CUDA=ON'\n",
    "!pip install llama-cpp-python[server]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Настройка окружения и импорт зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import base64\n",
    "\n",
    "# Проверка наличия всех необходимых секретов\n",
    "required_secrets = ['vds_host', 'ssh_key']\n",
    "missing_secrets = [secret for secret in required_secrets if not userdata.get(secret)]\n",
    "if missing_secrets:\n",
    "    raise ValueError(f\"Missing required secrets: {', '.join(missing_secrets)}\\n\"\n",
    "                    \"Add them in Colab: Left sidebar -> Key icon -> Add new secret\\n\"\n",
    "                    \"Required secrets:\\n\"\n",
    "                    \"- vds_host: IP адрес VDS\\n\"\n",
    "                    \"- ssh_key: SSH приватный ключ в base64\")\n",
    "\n",
    "# Создаем директорию .ssh и сохраняем ключ\n",
    "ssh_dir = os.path.expanduser('~/.ssh')\n",
    "os.makedirs(ssh_dir, mode=0o700, exist_ok=True)\n",
    "\n",
    "# Декодируем и сохраняем SSH ключ\n",
    "key_data = base64.b64decode(userdata.get('ssh_key')).decode()\n",
    "key_path = os.path.join(ssh_dir, 'tunnel_key')\n",
    "with open(key_path, 'w') as f:\n",
    "    f.write(key_data)\n",
    "os.chmod(key_path, 0o600)\n",
    "\n",
    "# Конфигурация из секретов\n",
    "VDS_HOST = userdata.get('vds_host')\n",
    "VDS_USER = 'tunnel'  # Фиксированный пользователь из скрипта установки\n",
    "LOCAL_PORT = 8000  # Порт для llama.cpp сервера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Создание модели для загрузки\n",
    "\n",
    "Загрузим модель Qwen с помощью HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "model_path = hf_hub_download(repo_id=\"Qwen/Qwen2-0.5B-Instruct-GGUF\", filename=\"*q8_0.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Запуск SSH туннеля и llama.cpp сервера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_print_output(process):\n",
    "    \"\"\"Helper function to continuously read and print process output\"\"\"\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "\n",
    "# Создание SSH туннеля\n",
    "ssh_command = f\"ssh -N -R 0.0.0.0:{LOCAL_PORT}:localhost:{LOCAL_PORT} -o StrictHostKeyChecking=no -i {key_path} {VDS_USER}@{VDS_HOST}\"\n",
    "ssh_process = subprocess.Popen(\n",
    "    ssh_command.split(),\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True\n",
    ")\n",
    "\n",
    "# Запуск llama.cpp сервера\n",
    "server_command = f\"python -m llama_cpp.server --model {model_path} --host 0.0.0.0 --port {LOCAL_PORT} --n_gpu_layers 35 --n_ctx 2048\"\n",
    "server_process = subprocess.Popen(\n",
    "    server_command.split(),\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True\n",
    ")\n",
    "\n",
    "server_thread = threading.Thread(target=run_and_print_output, args=(server_process,))\n",
    "server_thread.daemon = True\n",
    "server_thread.start()\n",
    "\n",
    "# Даем серверу время на запуск\n",
    "time.sleep(10)\n",
    "\n",
    "print(f\"\\n=== Your llama.cpp server is available at ===\")\n",
    "print(f\"http://{VDS_HOST}:{LOCAL_PORT}\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Shutting down...\")\n",
    "    server_process.terminate()\n",
    "    ssh_process.terminate()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
